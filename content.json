{"pages":[{"title":"about","text":"Github 안녕하세요 현재 컴퓨터학부 3학년에 재학 중인 주지호 입니다. 현재 관심있는 분야는 Cloud Computing, Backend 입니다. 또한, Data 분야에 관심이 가서 통계학을 부전공으로 공부하고 있습니다. “하루는 성실하게. 인생 전체는 되는대로” - 이동진(‘밤은 책이다’ 중에서) 라는 말씀을 제 인생의 방향으로 섬기며 살고 있습니다. 아직은 어떤 분야에 기여를 할 지 모르지만 미래에 사람들에게 들려줄 이야기가 많은 개발자가 되도록 오늘도 성실하게 달립니다!!!! Skills Python, C Django, Flask OpenStack Projects Contributon OpenStack OpenSource Project - 2021.09 ~","link":"/about/index.html"}],"posts":[{"title":"1 - Operating System Introduction","text":"들어가며평소에 개발자로 성장하기에 있어 Operating System 은 아주 중요하다는 얘기를 종종 듣게된다. 아직 현업에서 일해보지 않아 어떤 일을 하는데 있어 필요한지 경험을 해보진 못했지만 이번 OS 수업을 수강하며 최대한 이해하고 정리한 내용을 포스팅 하려고 합니다. 교수님의 수업 자료와 Operating Systems: Three Easy Pieces를 기반으로 정리하여 포스팅하려 합니다. Computer Science vs Engineering Science: 이론, 연구를 통해 정답을 도출하는 과정 Engineering: 주어진 환경 속에서 문제를 해결하는 과정 What is a Computer or Computer SystemApplication Software High Level Language 로 작성된다. (C/C++, Python, SQL …) System Software Compiler : HLL 을 Machine code 로 translate 해준다. Operating System : hardware 를 관리하는데 있어 다양한 concept 들을 제공해준다. (Window, Unix, Linux, DOS …) Handling input/output Managing memory &amp; storage Scheduling tasks &amp; sharing resources Computer Hardware Processor, Memory, I/O controllers What is Operating System왜 Why? OS 가 hardware 를 관리해주는가?? OS 역사를 봤을 때 초기엔 OS 가 아닌 컴퓨터 사용자가 직접 hardware 를 관리를 해줬다. 시간이 지나며 hardware 는 성능이 좋아지고 점점 더 복잡해졌다. 그로 인해 각 user 들이 관리하기 힘들어졌을 뿐만 아니라 직접 관리의 위험성이 커졌다. 그래서 hardware 를 관리해주는 OS 가 필요해지며 나타났다. Users applications 와 computer hardware 사이에 있는 Software layer 이다. ⇒ computer hardware 와 user applications software 를 interfacing 해준다. System call library : Hardware 가 관리하는 서비스를 사용. 즉, Hardware 와 통신하는 통로 ⇒ 안정성을 위함. Two Perspectives in Operationg SystemHardware 관점제한된 자원을 관리 Hardware Resources 를 효율적으로 관리 → maximize the performance ⇒ 한정된 자원으로 컴퓨터의 최대의 성능을 내기 위함 Hardware Resources 를 공평하게 할당(분배) → guarantee user experience ⇒ 다양한 접근(Multiprogramming)에 대해 공정하게 자원들을 분배 guarantee user experience? : 만약 한 자원에 여러 access 가 있을 시 한 access 요청이 끝날 때 까지 자원을 할당해주게 되면 뒤에 있는 다른 access 들은 아무것도 안하고 기다리고 있을 수 있다. 이러한 현상을 방지하기 위해 Round Robin 방식으로 자원을 공정하게 할당해주면 다양한 접근들에 효율적으로 자원을 분배할 수 있을 것이다. Applications 관점Computer System 을 편리하게 사용하게 함 User program 을 실행해주고 User Problems 을 해결해준다. A role of Operating Systems 각 사용자가 하드웨어 자원들을 독점하게끔 느낄 수 있도록 illusion 을 제공한다. Virtualization 복잡한 하드웨어 구조들을 숨겨주면서 사용자들이 컴퓨터 하드웨어를 편리하게 사용하게끔 해준다. Abstraction Three Main Features of Operating Systems현대 OS는 3개의 목적을 위해 다양한 기법들이 개발되었고 현재에도 개발되고 있다. 이 책에서는 3가지 목적을 위해 OS가 등장했다고 소개하고 있다. 그럼 어떤 목적이 있고 각 목적이 어떤 담당을 하는지 간단하게 정리해보겠다. 이번 글은 각 개념을 소개하는 글이므로 추후에 각 개념에 대해 자세히 정리해서 포스팅 하겠습니다:) Virtualization CPU Virtualization (Scheduling, Process, …) 한 개의 CPU(or 프로세서)에서 여러 개의 프로그램들이 동시에 실행되는 것처럼 보이게끔 해준다. 사용 시스템이 여러 개의 CPU를 가지고 있다는 illusion(환상)을 준다. Memory Virtualization (Paging, Address Translation…) 각 프로그램(프로세스)들이 자기만의 크고 빠른 메로리를 가지고 있다는 illusion을 준다. 실제 Physical memory 는 OS에 관리되는 한정된 공유 자원이다. 이 자원을 메모리 간섭 없이 각 프로세스가 자신만의 가상 주소 공간을 가지고 있다고 illusion(환상)을 준다. Concurrency동시에 프로그램들을 여러 개 수행해도 한 프로그램을 실행할 때와 동일하게 결과로 처리하겠다는 개념이다. Threads, Locks and semaphores, Deadlocks… Persistence이 개념은 “신뢰성” 이란 단어로 표현 가능할 거 같다. 예를 들어 과제를 하고 저장을 했다. 이 과제를 언제든지 불러오고 수정하고 다시 저장하는 과정을 오류없이 할 수 있어야하기 때문에 이러한 관점에서 persistence 라는 개념은 신뢰성을 의미한다. 만약 10시간을 해서 끝마친 과제를 발표를 위해 불러오려고 했는데 없어지거나 내용이 바뀌면,,,? 상상도 하기 싫다… File System I/O Devices Journaling OS가 어디에 사용되는지와 목적에 따라 3가지 개념의 중요도가 달라진다. → 현대에는 3가지 개념이 없는 OS는 없다.","link":"/2021/09/03/OS-0/"},{"title":"2 - History of Operating System","text":"들어가며OS에도 역사가 있다. Operating System Introduction에도 적어놨듯이 컴퓨터가 최초로 나왔을 때부터 OS가 있었던 것은 아니었다. 필요에 의해 지금의 OS가 개발된 것이다. 그럼 어떤 필요에 의해 어떤 OS의 기능들이 생겨 현재 OS가 탄생했는지 정리해보려 한다. Serial processing ~ 1950s이 시기는 하드웨어는 무척 비쌌지만, 인건비는 매우 쌌다. 그래서 천공카드니 뭐니 사람이 하드웨어를 직접 관리한 시기다. 당연히 이때는 OS가 존재하지 않았으니,,, 이 시기에 목적은 하드웨어의 효율적인 사용이다. 정확히 말하면 하드웨어가 쉴 틈이 없이 동작하게 하는 것이 목적이었다. 특징 Objective: 하드웨어를 효율적으로 사용 No operating systems (Single) Batch processing ~ 1960sBatch processing?? 이 개념이 무엇이고 대체 왜 도입이 됐는지 확인해보자. 1번 시기의 목적은 하드웨어의 효율적인 사용이다. 그리고 사람이 하드웨어를 관리하는 시기다. 다음 [그림-1]을 보면 각 job이 시작되기 전에 setup 하고 끝나면 takedown 을 하는 시간이 소요가 되는 것을 볼 수 있다. 이 시기의 목적이 하드웨어의 효율적인 사용인데 [그림-1] (a)를 보면 비싼 하드웨어가 놀고 있는 시간이 많은 것을 볼 수 있다. 그래서 각 job 들 중간에 있는 overhead 를 줄이기 위해 나온 것이 Batch 이다. Batch processing : 일괄 처리하는 과정으로 실시간으로 요청에 의해 처리되는 방식이 아닌 위와 같이 모든 job(프로세스)들을 처리하는 방식이다. 이 시기의 주 목적은 setup 과 takedown 시간을 줄여서 CPU utilization 을 향상시키는 것이다. Batch processing을 통해 [그림-1] (b)를 보면 어느정도 문제를 해결한 것으로 보인다. 하지만 다음 [그림-2]을 보면 여전히 CPU utilization이 나쁘다는 것을 볼 수 있다. 12345Read one record from file : 15 usExecute 100 instructions in processor : 1 usWrite one record to file : 15 usTotal : 31 usPercent CPU utilization = 1/31 = 0.032 = 3.2% 메모리 읽고 쓰는 속도보다 CPU의 속도가 훨씬 빠르니 이러한 문제는 당연하다. 그럼 [그림-2]의 문제를 해결하는 것은 생각? 이론 상으로는 간단하다. I/O 동안 CPU를 다른 곳에 쓰면 되는 것이다. 그래서 나온 개념이 Multiprogramming batch processing 이다. Multiprogramming batch processing 1960s~1970s2번 시기에 setup 과 takedown 시간을 줄여 CPU utilization을 향상시켰지만 그럼에도 여전히 CPU utilization은 나빴다. 그래서 이를 해결하고자 I/O로인한 CPU가 쉬는 시간을 없애고 다른 곳에 쓰이도록 하는 Multiprogramming batch processing 이란 개념이 나왔다. 위의 [그림-3]처럼 I/O 기간에 다른 Job 들을 처리하면 CPU utilization이 극대화될 것이다. 이를 위해선 해당 Job들이 미리 메모리에 올라와 관리되야한다. 이렇게 하는 이유는 CPU가 각 Job들을 사이에서 switch 할 수 있게 하여 가능한 CPU 가 최대한 바쁘게 만들기 위함이다. 즉, CPU utilization 향상을 위해!! 해당 Job(task, process) 들이 미리 메모리에 업로드 되야하니 Memory Relocation 이란 개념이 나타났고 서로 메로리를 간섭하면 안되니 Memory Protection 이란 개념이 나타났다. 그럼 Job 1이 수행되다가 I/O가 호출되어 CPU는 switch를 했는데 어떤 Job을 다음에 수행해야하지?? 해서 나온 개념이 Process Scheduler 이다. 다시 말하지만 이 시기의 주 목적은 느린 I/O 동작을 최대한 숨기고 overlap of computation with asynchronous I/O 하여 CPU utilization을 향상시키는 것이다. (영어로 쓴 건 한국어로 번역하게 되면 의미가 이상해 보여서입니다. ㅠㅠ) I/O 종류 Synchronous I/O CPU가 Job을 처리하는 도중 I/O 호출 → CPU는 I/O 동작이 끝날 때까지 기다린다. Asynchronous I/O 위와 반대로 CPU가 Job을 처리하는 도중 I/O 호출이 되면 context switch를 통해 다른 작업을 수행한다. Memory relocation [그림-4]에서 왼쪽은 가상 메모리 이다. 이 가상 메모리를 실제 Physical 메모리에 넣는 것이 memory relocation 이다. 이 과정은 목적에 맞는 다양한 정책과 결정이 필요하며 OS가 처리한다. Base/bound registersMMU(Memory Management Unit)의 초기 형태 Base registers: 실제 Physical 메모리 시작 주소를 담고 있다. Bound registers: Job(프로세스)에 할당된 공간양? 용량을 담고있다. 하드웨어의 지원으로 어떻게 하면 이 것을 최적화 시킬까??를 고민하고 나온 것이 현대적 OS 이다. 예를 들어 레지스터 2개가 지원이 된다 → 오 그럼 시작 주소랑 메모리 용량 2개를 저장해서 memory relocation 을 최적화 하면 되겠다. Memory protection [그림-7]과 같이 각 Job 들끼리 간섭이 일어나면 안된다!! Concurrency and Synchronization해당 글은 OS의 역사를 주제로 정리한 글이니 자세한 내용은 추후에 정리하도록 하겠습니다. **”Concurrency를 보장한다”**는 간단히 말하자면 동시 여러 Program끼리 간섭이 되지 않는 것을 보장한다와 같은 말이다. 만약 공유되는 counter 라는 변수를 증가시키는 명령문이 있다고 가정해보자. 메모리로부터 counter 라는 변수의 값을 레지스터에 Load 시켜줘야한다. 이것을 1 증가시킨다. 이것을 다시 메모리에 store 해준다. 이렇게 3개의 명령문은 atomically 하게 실행되는 않아 실행되는 과정에서 Concurrency를 보장해주지 않는다고 가정하면 다음과 같은 문제가 발생한다. 각 프로세스에서 1을 한번씩 더하면 값은 8이 나와야 하는데 결국 값이 7이 됐다. 좀 더 자세히 설명을 덧붙이자면 Process A 에서 counter=6을 업로드하고 1을 증가시켜주기 전에 갑자기 I/O호출이나 time slicing 시간이 끝나서 Process B로 switch 된다. 아직 Process A 에서 counter 의 값을 1 증가시켜주고 이를 메모리에 저장하는 과정을 수행하지 않아 Process B는 counter=6을 메모리에서 업로드 한다. 이 과정 때문에 그림과 같이 Concurrency 에 대한 에러가 발생한 것이다. 이는 여러 정책과 기법으로 해결을 해줘야 한다. 이는 추후에 다시 정리하도록 하겠다. 이를 Concurrency and Synchronization이 보장되지 않는 것을 현실 문제로 생각해보자. SMS 앱과 메일 앱이 있고 같은 시간에 메세지 앱에 “안녕하세요” 메세지가 왔고 메일 앱에 “하이요!” 메일이 왔다고 가정하자. 만약 이 두 내용이 메모리에 write 될 때 간섭이 발생하게 되면 “안하녕하이세!요” 같이 데이터가 충돌이 날 수 있다. 마찬가지로 은행 계좌를 생각해보면 concurrency 가 보장이 안되면 끔찍한 일들이 발생할 것이다. Time-sharing processing 1970s~1980s이제 이 시기로 오면서 CPU, 하드웨어에 초점이 맞춰졌다면 이제는 각 사용자가 어떻게 느끼는지? 또한 중요한 고려 요소가 되면서 이 시기의 주 목적은 Response time을 최소화시키는 것이다. Time-sharing processing은 Batch-multiprogramming system의 논리적 확장이다. CPU가 여러 Job들을 빈번하게 switch 하면서 multiple user들이 각 job들과 상호작용할 수 있게함으로써 interative computing(sharing the computer hardware resource) 이란 개념이 생겨났다. 하지만 이 개념은 illusion 을 주는 것이다. 예를 들어, single CPU 라고 가정 시 동시에 4개의 프로그램들이 running 하고 있다고 가정해보자. time sharing 을 통해 다음과 같은 순서대로 실행되지만 실행되는 속도는 매우 빨라 4개의 프로그램들이 겉보기에 동시에 실행되는 것처럼 보여 사용자 입장에서는 실시간으로 결과가 나오는 줄 알고 착각하게된다. 즉, CPU의 개수가 여러 개라는 환상(illusion)을 제공해주는 것이다. 근데 만약 한 연산(Job, 프로세스)이 I/O는 없고 연산할 것이 엄청 많다면 CPU는 한동안 그 작업만 처리해야하고 다른 작업은 처리를 할 수 없게 되어 다른 작업의 response time 이 올라가게 되어 사용자 경험이 안좋아지게될 것이다. 그래서 나온 개념이 Time Slicing 으로 공평하게 각 작업들에게 시간을 줘서 Round Robin 방식(자세한 내용은 추후에 정리)으로 스케줄링 시켜준다. ⇒ OS는 한정된 자원을 공평하게 분배를 하기 위함이다. 이렇게 Response time을 최소화시켜 User experience를 향상시키는 것이 이 시기의 주 목적이다. Real time System이 개념은 **”Fast”**와 무관하다. 정확한 개념은 주어진 시간동안 이 작업을 꼭 끝낼 수 있느냐? 이다. 즉, “Dead Line을 지킬 수 있느냐 없느냐” 이다. 예를 들면 1+1 요청의 응답이 1초 안에 올 수 있도록 보장할 수 있느냐의 약속이라고 생각하면 될 것이다. Hard real-time system 이 데드라인을 어기면 사람이 죽을 수 있을 정도로 정말 데드라인을 꼭 보장해야하는 시스템 (예) 비행기 시스템 Soft real-time system 어겨도 되긴 되지만 어기면 사용자들이 짜증이 나고 service 퀄리티가 떨어진다. QoS(Quality of Service): 다른 응용 프로그램, 사용자, 데이터 흐름 등에 우선 순위를 정하여, 데이터 전송에 특정 수준의 성능을 보장하기 위한 능력으로 사용자 경험에 영향을 많이 받음과 동시에 주기도 한다. 맺으며Operating System 은 내가 태어나기도 전부터 나타난 거라 아무런 생각 없이 당연하게 사용해왔다.(사용했다기보다 아예 신경을 안 썼다) 개발을 하면 항상 “그거 왜 사용해?”라는 질문을 많이 받곤 한다. 역사적으로 왜 생겨났으며 어떻게, 어디에 사용되는 지에 대한 설명이 위 질문에 가장 좋은 대답이라 생각한다. 이번 글을 포스팅하며 OS가 어떻게 나타나고 현대 컴퓨터에 왜 없어서는 안 될 시스템인지 공부할 수 있었다.","link":"/2021/09/10/OS-1/"},{"title":"OpenStack 이란 무엇인가??","text":"들어가며이번 2021 컨트리뷰톤프로그램에 프로그램에 참여하게 되었습니다. 10주동안 오픈소스 프로젝트 가이드와 함께 오픈소스 contribution을 직접 경험하는 프로그램 입니다. 제가 참여하는 프로젝트 주제는 OpenStack 입니다. 앞으로 10주동안 OpenStack 프로젝트에서 배우는 내용들이나 기여하며 배운 내용들을 이 블로그에 게시하고자 합니다. OpenStack 팀 업스트림 컨트리뷰션 Home: https://play.openstack-kr.org/pages/viewpage.action?pageId=12943363 OpenStack 이란??정의OpenStack 은 한 Datacenter 의 compute, storage, networking의 큰 영역을 관리하는 “cloud operating system” 이다. 이 시스템은 일반적인 인증 체계와 함계 API 를 통해 관리된다. 간단히 말하자면 “클라우드 환경에서 컴퓨팅 자원과 스토리지, 네트워크 인프라를 셋업하고 구동하기 위해 사용하는 오픈 소스 소프트웨어 프로젝트의 집합” 이다. 다른 정의들 OpenStack 은 공용 (Public) 클라우드와 사설 (Private) 클라우드 구축을 가능하게 하는 오픈 소스 소프트웨어 서버, 스토리지, 네트워크들과 같은 자원들을 모두 모아, 이들을 제어하고 운영하기 위한 클라우드 Operating System OpenStack 은 오픈 소스를 기반으로 클라우드를 구축하고 운용하고자 하는 오픈 소스 개발자, 회사, 사용자들이 주축이 되어 발전하는 커뮤니티 IaaS 형태의 클라우드 컴퓨팅 오픈 소스 프로젝트로 컴퓨팅, 스토리지, 네트워킹 자원을 관리하는 여러 개의 하위 프로젝트들로 이루어짐 나타나게된 이유 (왜 openstack을 개발했는지)OpenStack 은 2010년 NASA와 Rackspace가 하던 프로젝트를 오픈소스화하면서 처음으로 Austine 이라는 이름으로 릴리지를 하였다. 2010년도쯤만 해도 오픈소스 기반 Infrastructure 가 없어 큰 비용을 지불하며 대부분 vendor 사의 상용 솔루션을 사용했다. 예를 들어, 우리가 회사에서 가상 머신을 써야한다면 상용 솔루션으로는 VMWare 를 많이 사용하고 오픈소스 기반으로 한다면 단순 KVM 을 사용해 클러스터를 만드는 것에 그쳤다. 더 진보되서 Management, 가상 네트워크, Tenant 를 분리 등까지 가며 오픈소스 기반 Infrastructure 의 필요성이 대두되었다. 이렇게 해서 만들어진 것이 “OpenStack” 이다. OpenStack 을 이루는 Components현재 OpenStack 은 “Cloud Infrastructure for Virtual Machines, Bare Metals and Containers” 이다. 초기 오픈스택은 Virtual Machines 을 만드는데 주력했다. 그래서 초기 오픈스택은 compute, storage, image 로만 구성되어 있었다. 현재 OpenStack 은 6개의 핵심 컴포넌트들과 30여 가지의 기타 컴포넌트들로 구성되어 있다. 빨간색 Section이 Core 컴포넌트 역할(필수적 요소들을 담당)을 한다. nova (Computing Service) neutron (Networking) glance (Image Service) Cinder (Block Storage) swift (Object Storage) Keystone (Identity Service) ⇒ 6개 컴포넌트들이 Iaas 을 이루는 기본적인 기능(가상머신, 네트워크를 만들고 스토리지 제공)을 제공해준다. Compute (컴퓨팅) Nova (VM 인스턴스 관리) Storage (스토리지) Swift(Objects) Amazon 으로 따지면 S3 가 그런 역할 operation 단위가 block 단위가 아니라 object 단위 ⇒ file 단위로 operation 하는 Storage Cinder(Block) ( 가상 하드디스크 같은 읽고 쓰고 지우는 일반 디스크에 대한 자원 관리) Instance 들의 block device 같이게 있다. (Hardisk 와 같은 저장장치들) 왜 block storage? : 하드디스크나 ssd 같은 저장장치 운영체제에서 접근하면서 operation 을 내리는게 block 단위 Glance(Images) → 이것을 기반으로 VM 인스턴스를 만들기 위한 특정 이미지 관리(우분투, 윈도우 같은 이미지 관리) Identity (인증) Keystone tenant 즉 오픈스택에선 project 들의 계정에 대한 인증 체계 Network (네트워크) (Quantum) Neutron 웹 관리 포털 Horizon 서비스들을 쉽게 사용하기 위해 사용자들에게 Dashboard 를 제공하는 프로젝트 OpenStack 동작 방식오픈스택이 클라우드 인프라를 만드는 방법에 대해 정리하겠습니다. 동일 컴포넌트 내컴포넌트는 API 서버와 Agent 로 구성되어있다. API Server 가 클라이언트 혹은 다른 컴포넌트의 요청을 받아들인다. 예를 들어 nova 의 경우 인스턴스 생성이라는 요청을 처음 받는게 API Server 이다. API Server 는 REST API 를 제공해준다. ⇒ API Server 는 웹서버!! 뒷단에 있는 Agent 는 자원을 생성/삭제하는 역할을 한다. ⇒ Agent 는 자원(VM, Storage, Network)을 다룬다. 예를 들어 nova-compute agent 는 vm 을 생성/삭제한다. API Server 가 요청을 받아들이면 이 요청을 Agent 들에게 보내줘야 한다. 이 과정이 Agent 에 있는 함수를 호출하는 것이다. ⇒ RPC Call 이라 부른다. 예) nova API 가 인스턴스 생성 요청을 받으면 nova compute 한테 RPC Call 을 해서 인스턴스를 만드는 것이다. API 가 받은 요청을 Agent 가 처리할 수 있도록 명령하는 것 → rpc call 여기서 RPC 를 전달하는 방식을 오픈스택에서는 AMQP 를 지원하는 MQ 를 이용한다.(대표적으로 RabbitMQ) API Server 가 Queue 에 Message 를 넣으면 Agent 가 Queue 에서 가져가는 방식이다. ⇒ 동일 컴포넌트 내에서는 RabbitMQ 를 통해 RPC 통신을 주고 받는다. 다른 컴포넌트컴포넌트가 자기 스스로 모든 일을 처리할 수 없다. 예) nova-compute 가 vm을 생성하다가, vm에 연결할 block storage 생성을 cinder에 요청한다. 이럴 경우 Agent 가 다른 컴포넌트의 API 를 호출한다. 이 요청을 받은 컴포넌트는 또다시 RabbitMQ를 통해서 Cinder Volume 한테 요청해 storage volume 을 생성한다. 기본 동작 같은 컴포넌트 내에서는 RabbitMQ를 통해 rpc 통신을 한다. 다른 컴포넌트를 호출하기 위해선 해당 컴포넌트의 API Server 를 호출한다. 자원을 다루는 API는 대부분 비동기로 처리한다.인스턴스, 스토리지등을 만드는 과정 자체도 굉장히 복잡하기에 이 과정 하나하나가 Blocking I/O 로 처리가 된다면 굉장히 시간이 오래걸릴 것이다. Vm, Network, Storage 와 같은 자원을 다루는 API 는 모두 비동기로 처리된다. 자원 생성의 경우, API 서버는 (PUT)요청을 받고 자원의 UUID와 함계 202 Accepted를 반환한다. 자원을 요청한 쪽에서는 주기적으로 UUID 값으로 상태를 조회하며 생성여부를 확인한다. (timeout 시간 만큼 시도하다가, 확인되지 않으면 전체 요청을 실패처리한다.) OpenStack 의 자원 관리 방법 OpenStack 이 storage / network / vm 자체를 직접 만들지 않는다. OpenStack 이 자원의 상태만 관리하고 자원의 실체는 다른 서비스를 사용한다. 이런 이유때문에 오픈스택이 python 으로 충분히 동작을 할 수 있다!! OpenStack 실질직인 자원을 다루는 서비스와 연동하기 위해 다양한 Driver를 지원한다. nova 에서 지원하는 하이퍼바이저nova 는 실질적으로 인스턴스를 만들기 위해서 하이퍼바이저를 선택을 해줘야 한다. 결국 인스턴스를 만드는 기술들은 하이퍼바이저가 그 기술을 담당한다. 대표적으로 KVM, VMware 사용한다. OpenStack 의 전체적인 flow (인스턴스 생성) nova API 가 인스턴스 생성 요청을 받음 nova scheduler 라는 Agent 한테 요청이 들어온 vm 스펙대로 생성이 될 수 있는 하이퍼바이저를 선택해달라고 요청한다. ⇒ 즉, 스케줄링을 해달라는 것! 예) 4core 4GB 를 그거를 현재 만들 수 있는 하이퍼바이저를 찾아서 적절하게 선택을 해준다. 스케줄링이 되는 기준은 내부적으로 filter 를 거친다. (가용할 cpu로 쳐내고 메모리로 쳐내고 이미지로 쳐내고 기준을 정할 수 있다.) 기준대로 쳐내서 마지막으로 선택된 하이퍼바이저를 다시 nova API 에게 전달해준다. 그럼 Nova API 는 하이퍼바이저 정보를 알아낸 것! 그럼 하이퍼바이저(Compute node 라고 불름!) 에서 돌고 있는 Nova compute 한테 VM 생성 요청을 보낸다. ===== 현재 위의 모든 과정들이 rpc call(RabbitMQ) 를 통해 이루어지고 있다. ====== Nova Compute 는 요청을 받았으면 인스턴스가 만들어지기 위한 기본조건 2개가 필요 네트워크 스토리지 ⇒ 이 두개의 리소스들을 생성 요청을 해야한다. Neutron Server 와 Cinder API 한테 각각 vm에 사용할 네트워크 관련 정보 설정 요청, Block device 정보 요청을 보낸다. 이 두개의 컴포넌트들이 각각 비동기로 처리한다. Neutron Server 는 Compute node 에 있는 Neutron agent 한테 vm 에서 사용할 네트워크는 이 정도 내용이니 이것을 하이퍼바이저에 setting 하라는 명령을 보낸다. 예를 들어 openvswitch 를 쓰고 있다고 하면 이 것을 통해 하이퍼바이저 vm이 사용할 네트워크를 구성 openvswitch 에다가 VLAN 100 번을 쓸 수 있도록 네트워크 세팅을 하고 그 인스턴스가 사용할 인터페이스들을 다 만들어준다. Cinder 도 NAS를 사용한다고 하면 NAS 에다가 Volume 파일을 생성하고 block device 를 만들어 놓는다. 8번 과정에서 주기적으로 nova compute 는 Neutron Server 와 Cinder API 에게 계속해서 네트워크가 다 만들어졌는지, 블록 디바이스가 만들어졌는지 확인한다. → timeout 시간동안에!! timeout 시간 안에 이 리소스들이 다 만들어졌다면 비로소 인스턴스를 만든다. KVM 을 쓴다면 리포트 API 를 통해서 실질적인 vm 을 생성한다. 만약 위의 과정에서 네트워크는 성공적으로 만들어졌는데 Cinder 에서 block storage 를 더이상 생성할 용량이 없다면 Volume 의 UUID 상태를 fail 로 바꿔버린다. ⇒ 그럼 nova 는 인스턴스 생성을 못하니 최종적으로 인스턴스 상태를 fail 로 바꿔버린다. 그래도 성공적으로 생성된 네트워크는 즉시 삭제가 되지 않는다! 일단 남겨두고 있다가 인스턴스 삭제 요청이 들어오면 이 때 이 인스턴스와 관련있는 정보들을 삭제하고 수거한다. 이것이 openstack 이 동작하는 전체적인 방식이라고 생각하면 된다.","link":"/2021/08/15/openstack-0/"},{"title":"Master Challenge - openstack server list 명령어 동작 원리 파악하기","text":"들어가며“openstack server list” 명령어를 입력하면 코드상에서 어떻게 명령어의 인자 값을 구별하고 어떻게 처리를 하는지, 어떻게 예쁘게 테이블 형태로 출력하는지 동작하는지 정리해보겠다. 이 글에선 결론과 간략하게 도출과정을 소개하겠습니다. 상세한 도출 과정은 먼저 오픈스택 팀 블로그에 올려둔 상세 정리본에 작성했으니 참고하시면 되겠습니다. 또 미션 1, 2번의 경우 도출과정이 너무너무 길어 노션에 더욱 상세히 정리했으니 참고하시면 될 거 같습니다 :) OpenStack 팀 블로그: https://openstack-kr-contribution-academy-2021.readthedocs.io/ko/latest/ 인자로 입력받은 server list 를 어떻게 구별해내는가결론OpenStackShell object 는 주어진 인자(예: server list)를 처리하기 전에 각 API version 에 대한 혹은 cli, common, extension 등과 같은 group 이 추가가 되고 각 group 에 해당하는 command 들이 dict 형식(key: “server list”, value: serverlist에 대한 EntryPoint object)으로 OpenStackShell object 에 업로드 된다. 그 후 주어진 인자 값이 해당 객체 내 command dict 에 존재하는 지 확인한다. 도출 과정1234def initialize_app(self, argv): self._load_plugins() self._load_commands() self._load_plugins() command 수행하면 각 PLUGIN_MODULES 에 해당하는 command_group 과 command 들을 CommandManager obejct 에 사전 타입으로 업로드한다. PLUGIN_MODULES 는 다음과 같다. [openstack.cli.base] compute = openstackclient.compute.client identity = openstackclient.identity.client image = openstackclient.image.client network = openstackclient.network.client object_store = openstackclient.object.client volume = openstackclient.volume.client 업로드 되는 command 는 key 값에 명령어 name, value 에는 EntryPoint 객체가 할당된다. key: “server list” value: EntryPoint(name=’server_list’, value=’openstackclient.compute.v2.server:ListServer’, group=’openstack.compute.v2’) self._load_commands() command 를 통해 group_list 에 “openstack.common”, “openstack.extension” 이 추가가 되었고 그에 해당하는 command 들이 추가되었다. 1234567class App(object): ... def run_subcommand(self, argv): try: subcommand = self.command_manager.find_command(argv) except ValueError as err: ... find_command(argv) : App 클래스 find_command 메소드를 통해 주어진 server list 인자를 현재 OpenStackShell 에 포함된 command 에 있는 지 확인(구별)한다. 만약 인자로 주어진 명령어가 object 내의 command 에 존재하지 않으면 비슷한 (match 되는) 명령어 리스트를 보여주거나 에러를 발생시킨다. server list 라는 명령어를 처리하는 파일은 무엇인가?결론server list 명령어를 처리해주는 파일은 openstack/python - openstackclient/openstackclient/compute/v2/server.py 이다. 도출과정1번 문제에서 command 가 CommandManager 에 사전 형태로 업로드 되는 것을 알 수 있었다. 만약 “server list” 라는 명령어가 업로드된 command 안에 키 값으로 있다면 value 값을 리턴해준다. 이 value 는 EntryPoint 인스턴스이다. 이 EntryPoint 인스턴스는 name, value, group 에 대한 정보를 가지고 있다. “server list”의 경우 name: “server_list” value: “openstackclient.compute.v2.server:ListServer’” group: “openstack.compute.v2” server list 에 해당하는 EntryPoint 인스턴스 value 값에 이 명령어를 처리할 해당 파일과 객체의 정보가 있는 것을 볼 수 있다. openstackcli 는 어떻게 nova api 주소를 알아내나요?도출과정123456# site-packages/keystoneauth1/identity/v3/base.pydef get_auth_ref(self, session, **kwargs): ... resp = session.post(token_url, json=body, headers=headers, authenticated=False, log=False, **rkwargs) ... token_url: ‘http://&lt;오픈스택 구축 설정 ip 주소&gt;/identity/v3/auth/tokens‘ 오픈스택 구축 설정 ip 주소의 경우 저의 경우 클라우드 환경에 가상 서버를 구축하고 가상 서버 공인 ip 주소를 통해 오픈스택이 외부망과 통신할 수 있게끔 설정해줬다. 클라우드 상에서 오픈스택 구축 시 네트워크 설정 참고 자료 - 멘토님 블로그[https://printf.kr/14] OS_AUTH_URL: http://211.37.148.128/identity → 이 설정이 뭔지 설명하기 body: ‘auth’ 정보 headers: {‘Accept’: ‘application/json’} 1234# site-packages/keystoneauth1/session.pydef post(self, url, **kwargs): return self.request(url, 'POST', **kwargs) 1234567891011121314151617# site-packages/keystoneauth1/session.pydef request(self, url, method, json=None, original_ip=None, user_agent=None, redirect=None, authenticated=None, endpoint_filter=None, auth=None, requests_auth=None, raise_exc=True, allow_reauth=True, log=True, endpoint_override=None, connect_retries=None, logger=None, allow=None, client_name=None, client_version=None, microversion=None, microversion_service_type=None, status_code_retries=0, retriable_status_codes=None, rate_semaphore=None, global_request_id=None, connect_retry_delay=None, status_code_retry_delay=None, **kwargs): ... resp = send(**kwargs) ... return resp resp 는 kwargs(headers, auth 정보)를 가지고 ‘http://&lt;오픈스택 구축 설정 ip 주소&gt;/identity/v3/auth/tokens‘ 에서 다음과 같은 “catalog” 값으로 nova, keystone, cinder, glance 등 모든 컴포넌트들의 api 주소를 가져온다. 123# resp 중 &quot;catalog&quot; nova 정보&quot;catalog&quot;:[ ... {&quot;endpoints&quot;: [{&quot;id&quot;: &quot;e7507720bc274e56b420466613be3f07&quot;, &quot;interface&quot;: &quot;public&quot;, &quot;region_id&quot;: &quot;RegionOne&quot;, &quot;url&quot;: &quot;http://211.37.148.128/compute/v2.1&quot;, &quot;region&quot;: &quot;RegionOne&quot;}], &quot;id&quot;: &quot;3e7dec3e86ea4652ad633484b07fa368&quot;, &quot;type&quot;: &quot;compute&quot;, &quot;name&quot;: &quot;nova&quot;}, ... ] nova 의 어떤 API를 호출하여 결과를 받아오나요? ( 어떤 URI 를 호출하나요? )결론‘http://&lt;오픈스택 구축 설정 ip 주소&gt;/compute/v2.1‘ 를 호출해서 결과 값을 받아온다. 도출 과정3번에서 resp 안 “catalog”에 저장된 nova 정보를 보면 “url” key 의 value 값으로 nova URI 가 저장되어있다. 결과를 이쁘게 table 형식으로 출력해주는 함수는 무엇일까요?결론site-packages/cliff/formatters/tables.py TableFormatter 클래스의 emit_list 메소드 에서 결과를 이쁘게 table 형식으로 출력해준다. 도출과정server list 인자의 openstackclient/compute/v2/server.py 파일에서의 처리 결과로 column_names 와 data 값을 할당 받는다. column_names=(〈ID〉, 〈Name〉, 〈Status〉, 〈Networks〉, 〈Image〉, 〈Flavor〉) data = 해당 명령어 맞는 값들이 들어있는 generator 이다. 123456789class DisplayCommandBase(command.Command, metaclass=abc.ABCMeta): def run(self, **parsed_args**): parsed_args = self._run_before_hooks(parsed_args) self.formatter = self._formatter_plugins[parsed_args.formatter].obj column_names, data = self.take_action(parsed_args) column_names, data = self._run_after_hooks(parsed_args, (column_names, data)) **self.produce_output(parsed_args, column_names, data)** return 0 위에서 도출된 값들을 self.produce_output(parsed_args, column_names, data) 를 수행하며 결과값들이 출력이된다. 여기서 self 는 server list 인자 기준 ListServer object 를 가리킨다. 12345678910111213class Lister(display.DisplayCommandBase, metaclass=abc.ABCMeta): ... def produce_output(self, parsed_args, column_names, data): ... **self.formatter.emit_list( columns_to_include, data, self.app.stdout, parsed_args, )** return 0 self.formatter.emit_list(columns_to_include, data, self.app.stdout, parsed_args,) 를 수행 시 cliff/formatters/table.py/TableFormatter 에 emit_list 메소드가 호출된다. 123456789101112131415161718192021def emit_list(self, column_names, data, stdout, parsed_args): # column_names로 PrettyTable 객체를 생성해 x에 할당 x = prettytable.PrettyTable( column_names, print_empty=parsed_args.print_empty, ) x.padding_width = 1 # 이 값을 변경해보면 table 형식이 달라진다는 것을 알 수 있다. if data: self.add_rows(x, column_names, data) # 데이터들이 table의 각 행에 입력된다. min_width = 8 self._assign_max_widths( stdout, x, int(parsed_args.max_width), min_width, parsed_args.fit_width) formatted = x.get_string() stdout.write(formatted) # 결과값(테이블)을 출력해준다. stdout.write('\\n') return 해당 메소드에서 column_names 와 data 를 테이블 형식으로 만들어 출력까지 수행한다. 맺으며..OpenStack는 엄청 거대한 오픈소스 프로젝트이다. 이제까지 한번도 오픈소스 프로젝트를 건드려본 적이 없었다. 이번 기회에 어떻게 오픈소스 코드를 분석하며 동작원리를 파악하는지를 배웠다. 정말 인내심의 한계가 찾아왔었지만 계속 보고 또 보니 큰 그림이 그려지면서 뭐가 뭔지 파악이 되고 동작원리가 이해되기 시작했다. 정말 이 프로젝트를 하며 너무너무 많은 것들을 배우는 거 같다^_^","link":"/2021/08/22/openstack-2/"},{"title":"Master Challenge - CLI와 친해지기","text":"들어가며저번 주엔 devstack 을 구축했다면 devstack 안에서 오픈스택 CLI 를 다뤄보는 것이 이번 주 미션이었다. 사실 아직 openstack에 대해 제대로 이해도 못해서 과제를 과연 잘 끝낼 수 있을까? 두려움이 앞섰지만 막상 시작해보니 아주 잘 끝냈다ㅎㅎ 역시 “시작이 반이다” 라는 말이 괜히 있는게 아닌 거 같다. 이제 어떻게 이 미션을 클리어 했는지 정리해보겠습니다. 아 그리고 먼저 팀 블로그에 더 상세히 정리한 내용이 있으니 오픈스택 팀 블로그에 올려둔 상세 정리본 부족한 내용은 여기서 보충해주시면 좋을 거 같습니다^^ OpenStack 팀 블로그: https://openstack-kr-contribution-academy-2021.readthedocs.io/ko/latest/ 환경변수 설정먼저 오픈스택 command를 사용하기 위해 환경변수를 설정해주자. 만약 환경변수 설정을 안하고 openstack command 를 사용하게 되면 다음과 같은 ERROR를 보실 수 있을 것이다. 그렇기에 꼭 환경변수 설정부터!! Missing value auth-url required for auth plugin password devstack은 환경변수를 사용자 계정에 맞게 설정해주는 스크립트를 제공해준다. 1source openrc admin(사용자) admin(프로젝트이름) 해당 command는 환경변수에 필요한 내용을 admin 사용자의 권한으로 읽어오고 admin 프로젝트를 default 로 바라보게 해준다. 해당 command를 수행하면 다음과 같은 에러가 발생할 수 있다. CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead 원인 an upstream bug with twisted’s dependency 해결방안 crytography &lt; 3.4 와 같은 옛날 버전의 crytography 를 설치해야 한다. 이 버전은 twisted’s dependency를 만족하고 deprecation warings 를 throw 하지 않는다. cd /opt/stack/devstack 에서 pip install cryptography==3.3 명령어를 수행하니 더이상 에러가 발생하지 않는다. version 3.3 을 선택한 이유는 3.4 이하 버전에서 가장 최신 버전이라 선택했다. 그럼 이렇게 devstack 환경변수 설정을 끝냈으면 과제를 수행해보자! cirros image 로 인스턴스 생성을 cli로 해보기과제를 끝마치고 이 글을 정리하면서 드는 생각은 이 문제는 쉬웠다고 생각한다. 하지만 시작할 땐 cirros image?가 대체 뭐지,,,인스턴스 생성?? 일단 구글링을 해서 어느 분이 정리해준 블로그를 보며 답에 대한 실마리를 찾았다. 그리고 공식문서[https://docs.openstack.org/python-openstackclient/pike/cli/command-list.html]를 뒤져보며 정답을 찾았다. 역시 공식문서에 openstack command List 가 아주 상세히 정리되어있다. 12openstack server create --image=[가지고 있는 이미지] --flavor=[사양] --network=[public/private/shared] instance_nameopenstack [server, flavor, network] list # (server, flavor, image 등) list 출력 ubuntu 이미지를 받고, root password를 설정한 다음 cli로 이미지 등록한 후 인스턴스 생성하고 접속까지 하기Ubuntu Image 다운로드 받기OpenStack 용 우분투 이미지는 일반 OS 이미지를 사용하는 것이 아니라 클라우드용으로 생성해 놓은 이미지를 생성해놔야 한다. 원하는 이미지로 인스턴스를 생성하기 위해서는 이미지 파일을 받아서 오픈스택 대쉬보드에 업로드를 해야한다. 이미지 공금원 사이트 : Ubuntu Cloud Images[https://cloud-images.ubuntu.com/bionic/current/] (해당 링크는 OpenStack 공식문서 Ubuntu 18.04 LTS (Bionic Beaver) Daily Build 로 다운 링크가 걸려있다.) OpenStack 공식문서를 보게되면 다음과 같은 내용이 있다. 1If your deployment uses QEMU or KVM, we recommend using the images in qcow2 format, with name ending in .img. The most recent version of the 64-bit amd64-arch QCOW2 image for Ubuntu 18.04 is bionic-server-cloudimg-amd64-disk.img Ubuntu 18.04 is bionic-server-cloudimg-amd64-disk.img 를 다운 받을 것을 권장한다. 해당 image를 리눅스 명령어 wget 를 사용해 devstack 내부에서 다운받자. 1wget &lt;다운로드 URL&gt; root password 설정Ubuntu Cloud Image는 default username/password 가 없어 해당 image 로 instance 를 생성하기 전에 root password 설정이 필요하다. virt-customize 라는 툴을 이용하여 image root password 를 설정해주자. 그 전에 해당 cmd 를 사용하기 위해선 libguestfs-tools pkg 를 다운 받아야 한다. 1sudo apt install libguestfs-tools 위와 같이 pkg 를 설치했다면 다음과 같은 명령어로 root password 를 변경해보자 1sudo virt-customize -a bionic-server-cloudimg-amd64.img --root-password password:&lt;비밀번호&gt; 참고 https://docs.openstack.org/image-guide/obtain-images.html http://charmingwebdesign.com/how-to-set-a-root-password-for-your-openstack-images-2/ https://askubuntu.com/questions/451673/default-username-password-for-ubuntu-cloud-image cli로 image 등록하기1번 문제와 비슷하다. 해당 command로 image를 등록해주면된다!!! 12# 여기선 network 를 public 으로 지정했다.openstack image create &quot;Ubuntu-18.04 LTS&quot; --file bionic-server-cloudimg-amd64.img --disk-format qcow2 --container-format=bare --public 참고 https://docs.openstack.org/glance/pike/admin/manage-images.html https://docs.openstack.org/python-openstackclient/pike/cli/command-list.html 인스턴스 생성하기1번 문제와 같이 해당 command를 수행하게되면 openstack instance 가 생성된다. 12openstack server create --image=&lt;image_name&gt; --flavor=&lt;flavor_name&gt; --network=&lt;network_name&gt; &lt;instance_name&gt;# 예) openstack server create --image=Ubuntu-18.04 LTS --flavor=m1.tiny --network=public task2_instance cli로 floating ip 생성 후 인스턴스에 할당 / 해제 해보기Floating IP 는 Fixed IP 처럼 자동으로 인스턴스에 default 로 할당되어 있지 않기 때문에 직접 인스턴스에 attach 해줘야 한다. 사용자들은 external network 로부터 인스턴스에 대한 연결성을 보장해주기 위해 cloud administrator 에 의해 정의된 다른 pool 부터 floating IP를 grab 해와야 한다.[https://www.mirantis.com/blog/configuring-floating-ip-addresses-networking-openstack-public-private-clouds/] Floating IP 생성 후 인스턴스에 할당Floating IP Address는 Default로 public pool 로 부터 할당된다. 12openstack floating ip create public # floating ip를 클라우드 서버 인스턴스에 할당openstack floating ip list # floating ip 들 확인 해당 명령어를 통해 floating ip 가 생성된 것을 확인할 수 있을 것이다. 이제 생성된 floating ip address를 instance와 연결시켜보자. 12openstack server add floating ip &lt;instance_name&gt; &lt;floating_ip_address&gt;# openstack server add floating ip test_instance 생성한 floating ip address 를 instance 에 associate 해주는 과정에서 발생했다. ResourceNotFound: 404: Client Error for url: http://211.37.148.128:9696/v2.0/floatingips/d47567b0-9a41-4aec-b733-d9b0d6a3cf26, External network fe2d465c-4669-45de-8860-fa6373ef9ca2 is not reachable from subnet bcec19b3-1711-4af9-80e2-b1b2fdd95457 . Therefore, cannot associate Port 7a8c850c-152b-434b-b900-2206375fc0a4 with a Floating IP. Error 발생 이유 public 네트워크로 인스턴스를 생성 시 생성된 인스턴스는 Clound VM 의 가상 네트워크를 사용하는 것이 아닌 실제로 존재하는 네트워크 주소를 가진 것이다. 현재 생성한 Floating IP 는 public pool 에서 가져온 것(실존 네트워크)이니 굳이 이 Floating IP 를 public 네트워크로 생성한 인스턴스에 할당해줄 필요가 없다. 서로 실존하는 네트워크가 겹쳐? 에러가 난 것이다. 해결 방안 public 이 아닌 private 네트워크로 생성한 인스턴스에 floating IP를 할당해주도록 하자!!!! 참조 https://www.mirantis.com/blog/configuring-floating-ip-addresses-networking-openstack-public-private-clouds/ https://docs.openstack.org/ocata/user-guide/cli-manage-ip-addresses.html https://help.dreamhost.com/hc/en-us/articles/215912768-Managing-floating-IP-addresses-using-the-OpenStack-CLI 10.8.0.0/24 네트워크를 만들고 public network와 연결하는 과정을 cli로 해보기이제 오픈스택 CLI에 감?을 잡아서 자신감이 생겼다ㅎㅎ 그래서 CLI로 하기전에 GUI 로 네트워크를 생성하고 public network 와 연결하는 과정을 진행해보며 CLI로 하는 과정에서 어떤 옵션을 줘야하는지 파악하기로 했다. GUI 과정은 이 글에선 생략하고 바로 CLI 과정을 정리해보겠습니다. 오픈스택 팀 블로그]에 먼저 GUI 과정과 CLI 과정을 상세히 정리해놨으니 보충이 필요하다면 참고하시면 되겠습니다!! CLI 과정CLI 를 통해 생성해야 되는 것 &amp; 각 옵션들 network ◦ 네트워크 이름 subnet ◦ 서브넷 이름 ◦ 네트워크 주소 range ◦ 서브넷을 서브넷팅 해줄 네트워크 이름(위에서 생성한 네트워크 이름) interface ◦ router에 연결할 서브넷 이름 네트워크 생성1openstack network create &lt;네트워크 이름&gt; 해당 명령어를 수행하면 네트워크가 생성된 것을 확인할 수 있을 것이다. 아직 subnet를 지정해주지 않아 subnets 옵션에 값이 없는 것을 볼 수 있다. 서브넷 생성1openstack subnet create --network=&lt;연결 네트워크 이름&gt; --subnet-range=&lt;네트워크 지정 범위&gt; &lt;서브넷 이름&gt; Public network에 연결하기라우터에 연결할 Subnet의 인터페이스를 생성해야하니 다음 명령어를 수행해주자. 1openstack router add subnet &lt;라우터 이름&gt; &lt;서브넷 이름&gt; 그 결과 router에 해당 subnet 인터페이스가 생성된 것을 볼 수 있다. 참조 https://docs.openstack.org/newton/ko_KR/install-guide-obs/launch-instance-networks-selfservice.html https://docs.openstack.org/liberty/ko_KR/install-guide-obs/launch-instance-networks-private.html","link":"/2021/08/20/openstack-1/"},{"title":"1.5 - Schematic Overview of Computer System","text":"들어가며..OS를 깊게 공부하기 전에 computer system의 각 구성요소와 담당 기능들을 기반으로 정리해보려고 합니다. Von Neumann Machine현대에 모든 computer system들의 prototype은 Von Neumann Machine 이다. Von Neumann Machine에 기반한 컴퓨터는 다음 3가지가 충족되야 한다. Arithmetic / Logic Unit 이 필요 전체 제어를 할 수 있는 Control Unit 이 필요 CPU는 오직 메모리(주기억장치로 cpu가 처리해야 할 instruction, data를 저장할 공간)랑만 통신이 가능 어떤 프로그램을 CPU가 처리하게 하려면 해당 프로그램을 메모리에 저장(업로드)해줘야 한다. → Stored-program computer [그림 1]을 통해 메모리와 CPU는 어떤 것들을 통신하고 구성하고 있는 요소들이 무엇인지 알아보자. Central Processing Unit(프로세서 or CPU) Control Unit: 입출력 장친 간 통신 및 조율을 제어하며 명령어들을 읽고 해석하며 데이터 처리를 위한 시퀀스를 결정한다. Arithmetic / Logic Unit: 연산을 담당한다. Registers 중앙처리장치(CPU)는 오직 메모리와만 통신이 가능하다. register는 메모리로부터 가져온 프로세스의 data들을 저장하는 저장장치이다. 프로세스 실행 시 프로세스의 정보를 저장하고 이를 가지고 CPU는 연산을 수행한다. PC(Program Counter): 다음에 수행될 명령어의 주소를 가지고 있는 레지스터 MDR(Memory Data Register): 메모리와 CPU 사이의 데이터 교환 시 임시로 자료를 보관하는 레지스터로 계산된 데이터를 임시로 저장하는 역할을 한다. AC(Accumulator): 메모리로부터 읽어온 데이터와 AC에 저장되어 있던 데이터가 지정된 연산을 수행한 후 그 결과 값을 AC에 저장하는 누적 Accumulator이다. 예를 들면 1+2 연산 후 x3 연산을 수행한다고 가정하면 1+2=3 결과값 3을 AC에 저장 후 3x3=9 결과값을 다시 AC에 저장한다. MemoryDisk에 저장된 실행 가능 파일(프로그램)이 메모리에 업로드되면 그 프로그램은 프로세스가 된다. 프로세스는 Instruction과 Data들의 집합 이다. [그림 2]를 통해 위의 내용을 다시 간략히 정리해보자. Memory 는 CPU가 바라보는 가상 메모리 이다. 메모리는 Byte addressable array 로 불리는데 주소 하나가 1Byte를 의미하기 때문이다. 메모리 주소 0 → 1Byte, 메모리 주소 1 → 1Byte…로 array 형태를 가진다. Code, 유저 데이터, 몇몇 OS 데이터들을 가진다. Disk 에 있는 Executable Program(실행 가능 프로그램) 이 메모리에 올라가면 프로세스가 되며 CPU가 이 프로세스를 처리할 때 프로세스의 데이터들과 명령어들이 CPU 레지스터에 저장된다. 해당 instruction이 끝나면 PC(Program Counter)에 저장된 다음 instruction 주소에 해당하는 데이터와 instruction을 레지스터에 저장한다. Mode BitMode bit 는 Program Status Word (PSW) register(하드웨어)에 저장된 flag 이다. Mode bit 0 시스템이 kernel mode에 있다는 것을 표현 OS가 CPU(processor)의 권한을 얻는다. Mode bit 1 시스템이 user mode에 있다는 것을 표현 user application 이 실행 중 → user program 이 CPU 권한을 갖는다. OS는 이렇게 mode bit 을 세팅하여 Dual mode를 갖는다.Mode Bit 이 1인 경우 Interrupt, I/O, System call 등을 통해 user mode → kernel mode 로 바뀐다. 이렇게 Dual mode 갖는 이유는 OS 자체를 보호하고 다른 시스템 요소들(memory protection)을 잘못되고 불법적인 프로그램 실행으로부터 보호하기 위함이다. 즉, 안정석이 중요하여 현재 어떤 모드에 있는지 표현하여 명백하게 구별이 되야 한다. 현재 실행 중인 프로그램이 user data와 함께 너무 많은 메모리를 잡아먹어 OS를 나가떨어지게 할 수 있다. 멀티 프로세스의 경우 같은 시스템 안에서 동시에 좋지 않은 결과들을 write 할 수 있다. 실제 사용 예들은 밑의 다른 컴포넌트를 설명하며 자세히 정리해놨습니다. TimerTimer 의 주요 할 일은 특정 기간(시간) 이후에 CPU를 interrupt 시키는 것이다. 예를 들어 시간이 지남에 따라 counter 가 감소된다고 가정하자. counter 가 0에 도달하게 되면 interrupt 가 발생하고 CPU 제어권은 OS에 넘겨진다. 이를 time interrupt 라고 한다. 이는 어느 한 program이 CPU를 독점하는 것을 방지하기 위함이다. → time-sharing system user program 에 할당된 시간은 1초라고 가정해보자. program k 가 실행되는 동안 program k에 CPU 권한이 가있다. (mode bit: 1) 1초의 시간이 지나면 timer interrupt 가 발생하고 schedule 작업(다음에 어떤 task를 실행할지 결정)을 위해 CPU 권한을 OS에 넘겨준다. → 이를 위해선 user mode, kernel mode가 필요하며 또한 이를 위해 mode bit가 필요하다. mode bit 가 1에서 0으롤 바뀌며 위와 같은 작업을 반복해서 수행한다. I/O DeviceI/O(scanf)가 발생하면 CPU는 어디에 저장하라는 명령만 한다. Device controller가 이를 받아서 처리하며 Device controller의 행동을 권장하는 프로그램이 Device driver이다. I/O Device ControllerCPU와 역할이 비슷하다!! 컴퓨터의 input/output bus에 연결된 하드웨어 유닛으로 하드웨어에게 computer와 I/O device들 사이의 interface를 제공해준다. Local buffer ⇒ 시스템의 메모리 역할 I/O Processing이 끝난 후 device controller는 interrupt 신호를 CPU에게 보낸다. Device driverOS와 역할이 비슷하다!! 컴퓨터에 연결된 특정 타입 devic를 operate, control 하는 computer program이다. 하드웨어 device의 Software interface로 OS와 다른 컴퓨터 프로그램들이 정확한 세부사항들을 알 필요없이 하드웨어 기능(function)에 접근하게 해준다. ⇒ 즉, abstraction을 제공해주며 하드웨어 장치와 application, OS 사이에서 translator로서 작동한다. 드라이버는 흔히 컴퓨터 버스, 또는 하드웨어와 이어진 통신 하위 시스템을 통해 장치와 통신한다. 요청하는 프로그램이 드라이버의 명령어를 호출하면, 드라이버는 장치에 명령어를 전달한다. 장치가 드라이버에게 데이터를 되돌려 주면, 드라이버는 원래 요청한 프로그램의 명령어로 데이터를 다시 전달한다 ex) .sys in Window / .ko in Linux Input/Output(I/O) 과정 유저 Program으로부터 I/O를 요청(ex. scanf, printf) System call (user program 내) Mode bit이 1 → 0 (kernel mode)으로 변경된 후 OS가 CPU에 대한 권한을 얻는다. 요청 I/O data를 device controller에게 보낸다. (이 시간에 타당한 I/O요청인지 결졍된다. 예를 들면 process가 인증이 됐는지?) Mode bit 0 → 1 로 변경된 후 다른 프로세스에게 권한을 주고 CPU는 해당 프로세스를 실행한다. I/O Processing이 끝났다는 신호를 받으면 다시 interrupt 으로 현재 실행 중인 process를 stop 한다. 다시 Mode bit이 1 → 0 (kernel mode)으로 변경된 후 OS가 CPU에 대한 권한을 얻는다. OS는 device controller로부터 buffer 안에 저장된 data를 받는다. buffer data를 I/O를 요청했던 프로세스의 instruction의 메모리 or area에 저장한다. Mode bit 0 → 1 로 변경된 후 이전에 I/O로 인해 stop 된 프로세스에게 권한을 주고 CPU는 해당 프로세스를 실행한다. Exception(Interrupt)Exception(or Interrupt)는 몇몇 event로 인해 OS로의 권한 이동이다. 프로세스 상태가 바뀌면(event) mode bit은 0으로 바뀌게 되고 CPU 권한은 OS로 이동하게 된다. 이를Exception or Interrupt 이다. 위 [그림 5]과 같이 event가 발생하면 CPU는 무엇을 해야할 지 모르며 이를 혼자서 처리하지 못한다. 이를 OS가 처리를 해야하며 이를 해결하기 위해선 CPU의 권한을 받아야 한다.(커널 모드) return to I_current: 현재 명령어 줄로 되돌아간다. return to I_next: 다음 명령어 줄로 리턴한다. abort: 프로그램 전체를 Stop 시킨다. 이는 어떤 exception이 발생하는 지에 따라 다르다. Exception은 2종류가 있다. Asynchronous Exceptions : Interrupt외부 하드웨어 device에 의해 발생하는 Exception(ex. I/O, timer)으로 CPU는 exception이 언제 발생할지 예측하지 못한다. Exception Handler는 오직 “next” 명령을 리턴한다. Examples Timer interrupt 외부의 timer chip은 interrupt를 trigger 한다. user program으로 부터 CPU 권한을 가져와 kernel 모드에서 처리한다. I/O interrupt from external device 키보드에서 Ctrl-C를 눌렀을 때 네트워크로부터 패킷이 도착 I/O device들로부터 데이터가 도착 Synchronous Exceptions한 instruction 실행 결과로서 생기는 event에 의해 발생하는 exception으로 CPU가 혼자서 처리를 할 수 없어 **스스로 문제제기(trigger)**를 한다. ⇒ “software interrupt” or “internal interrupt” Examples Traps 의도적으로 exception을 발생시키는 것으로 이것만으로 문제가 되지 않는다. System calls, breakpoint traps, special instructions가 그 예이다. exception이 처리가 되면 exception이 발생한 “다음” instruction에 권한을 리턴한다. Intentional Examples: System calls, breakpoint traps, special instructions Returns control to “next” instruction Faults 의도적으로 exception이 발생하지 않지만 해결이 가능한 경우가 존재한다. Faults 가 발생하는 과정을 예를 들어 정리해보자. CPU는 메모리와만 통신이 가능하다. 이는 메모리에 필요한 data가 없는 경우 디스크에 있는 data를 메모리로 가져와야한다. a = 1 + 2 instruction을 수행하는 도중 2라는 data가 메모리에 존재하지 않아 page fault가 발생했다. disk에서 2를 가져와 메모리에 저장한 후 a = 1 + 2 명령어를 다시 수행하게 된다. 여기서 다른 exception과의 차이점은 다음 instruction이 아닌 현재 instruction을 다시 수행한다는 점이다. Unintentional but possibly recoverable Examples: page faults (recoverable), protection faults (unrecoverable), floating point exceptions (divide by 0), invalid memory access … Either re-executes faulting (“current”) instruction or aborts Aborts 의도적으로 exception이 발생하지 않으며 해결이 불가능하다. 명령어 자체에 오류가 생길 경우 프로그램 전체를 stop 시키고 프로그램을 처음부터 다시 실행해야 한다. Unintentional and unrecoverable Examples: illegal instruction, parity error, machine check Aborts current program Exception 정리 External(외부) exception: CPU가 예측할 수 없으며 외부 하드웨어에서 발생하며 이를 해결한 뒤 “next” instruction을 리턴한다. Internal(내부) exception: CPU가 혼자 처리를 할 수 없어 스스로 문제를 제기한다. Exception 처리(Handling) [그림 7]을 보면 각 exception 내용마다 부여된 exception number(index)가 있다는 걸 알 수 있다. 그럼 CPU에서 event 발생 시 어떤 exception이 발생했는지 OS에게 index 번호를 알려주어 OS는 어떤 이유로 stop이 됐는지 알게된다. 바로 위에서 각 event들은 자신만에 unique한 exception number를 가지고 있다고 했다. 그럼 어떤 event가 발생하면 Exception Table에서 해당 event의 exception number(index)에 해당하는 값을 찾을 수 있다. 해당 index의 값은 Interrupt handler 이다. Interrupt handler(Interrupt Service Routine, ISR) 는 특정 Interrupt에 관련된 code 블록이다. 쉽게 말하자면 “Event를 어떻게 해결해야 되는지에 대한 solution” 이라고 생각하면 된다. [그림 8]을 보면 exception이 발생해서 어떻게 이 exception을 처리하는 지 보여준다. device or CPU에 의해 interrupt가 요청된다. 현재 CPU의 현재 상태(register, program counter…)를 PCB에 저장하고 context switch를 한다. (PCB, context switch 는 추후에 정리) CPU는 interrupt vector(address of the ISR)를 살펴보고 그 주소를 PC에 넣는다. Interrupt Vector Table: memory location of an interrupt handler (0~255) CPU는 ISR(Interrupt Service Routine, Interrupt handler)를 동작하게 한다. Interrupt Service Routine: 각 interrupt를 어떻게 handling할 지 알려주는 kernel function으로 logical interrupt value 를 리턴해준다. 레지스터들의 값을 restore 해준다. ISR이 끝나면 IRET가 시작된다. IRET: 프로세서의 상태를 restore 해주는 return instruction. 기존의 프로그램으로 리턴된다. DMAscanf(system call)로 인해 키보드로 “hello world” 를 입력한다고 가정을 해보자. 만약 ‘h’ 가 입력되면 data가 입력됐다고 interrupt 걸고 다시 ‘e’를 입력하면 interrupt 걸고,,,타이핑 하나마다 data가 들어왔다고 interrupt 걸면 효율성이 많이 떨어진다. 그래서 data가 어느정도 size가 될 때까지 들어와 이정도면 됐어!라고 했을 때 한번에 메모리에 넣어주는 것이 더 효율적일 것이다. 이는 user mode 와 kernel 모드를 왔다갔다 하는 오버헤드를 최소화하기 위함이다. 이 역할을 하는 것이 바로 DMA(Direct Memory Access) 이다. DMA의 역할은 위에서 설명했듯이 외부 하드웨어 장비로부터 빈번한 interrupt가 발생되는 것을 줄이는 것이다. (Byte 단위가 아닌 Block 단위로 interrupt를 걸어준다.)","link":"/2021/09/17/OS-1-5/"}],"tags":[],"categories":[{"name":"OS","slug":"OS","link":"/categories/OS/"},{"name":"OpenStack","slug":"OpenStack","link":"/categories/OpenStack/"}]}